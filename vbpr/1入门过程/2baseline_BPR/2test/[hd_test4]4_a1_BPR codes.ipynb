{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[hd_test4]4_a1_BPR codes.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python2","display_name":"Python 2"}},"cells":[{"metadata":{"id":"_wq8HkapgntZ","colab_type":"code","outputId":"1f1f88cc-a1f5-4b37-902f-fdbbb2d2274f","executionInfo":{"status":"ok","timestamp":1552555379427,"user_tz":-480,"elapsed":19081,"user":{"displayName":"Jacob Ji","photoUrl":"https://lh3.googleusercontent.com/-zoRekIA1fCY/AAAAAAAAAAI/AAAAAAAACBQ/eV12vlBRQiA/s64/photo.jpg","userId":"06638368627777389358"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"cell_type":"code","source":["!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 授权登录，仅第一次的时候会鉴权\n","def login_google_drive():\n","  auth.authenticate_user()\n","  gauth = GoogleAuth()\n","  gauth.credentials = GoogleCredentials.get_application_default()\n","  drive = GoogleDrive(gauth)\n","  return drive\n","\n","# 列出id对应目录的所有文件\n","# \"q\" 查询条件教程详见：https://developers.google.com/drive/v2/web/search-parameters\n","def list_file(drive, dir_id_str):\n","  file_list = drive.ListFile({'q': dir_id_str+\" in parents and trashed=false\"}).GetList()\n","  for file1 in file_list:\n","    print('title: %s, id: %s, mimeType: %s' % (file1['title'], file1['id'], file1[\"mimeType\"]))\n","\n","def cache_data(file_id_str):\n","  # id 替换成上一步读取到的对应文件 id\n","  u_data = drive.CreateFile({'id': file_id_str}) \n","  \n","  #这里的下载操作只是缓存，不会在你的Google Drive 目录下多下载一个文件\n","  u_data.GetContentFile('u.data', \"text/csv\")\n","  print(\"缓存成功\")\n","\n","def load_data():\n","    titles = []\n","    print(\"正在加载数据...\")\n","    with open(\"u.data\", \"r\") as f:\n","        for line in f.readlines():\n","            titles.append(line.strip())\n","            \n","    print(\"一共加载了 %s 个标题\" % len(titles))\n","\n","    return titles\n","  \n","drive = login_google_drive()\n","#list_file(drive,\"'1DCdKTVKMGR_ei-NWE-68Y_mRFPf5RQhv'\")\n","cache_data(\"14Eha5chGz6RNWIYALvZBGI8Q-yAUmj7l\")\n","titles = load_data()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["缓存成功\n","正在加载数据...\n","一共加载了 100000 个标题\n"],"name":"stdout"}]},{"metadata":{"id":"aLLfh1YtgsyV","colab_type":"text"},"cell_type":"markdown","source":["## 备注：修改参数：\n","- BatchSize=256：尽量充分利用内存\n","- LR=0.02：下调学习速率，能更好抓住最优解，之后考虑加入动态调节学习速率的方法\n","- RR=0.001：\n","- hidden_dims = 60\n","- k=5000：充分调用数据\n"]},{"metadata":{"id":"7hbhWAoNgwum","colab_type":"code","outputId":"d6b2f7ec-cc21-4d37-da7e-9f7188f9bf8a","colab":{"base_uri":"https://localhost:8080/","height":3241}},"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import os\n","import random\n","from collections import defaultdict\n","\n","\n","def load_data():\n","    user_ratings = defaultdict(set)\n","    max_u_id = -1\n","    max_i_id = -1\n","    with open('u.data','r') as f:\n","        for line in f.readlines():\n","            u,i,_,_ = line.split(\"\\t\")\n","            u = int(u)\n","            i = int(i)\n","            user_ratings[u].add(i)\n","            max_u_id = max(u,max_u_id)\n","            max_i_id = max(i,max_i_id)\n","\n","\n","    print(\"max_u_id:\",max_u_id)\n","    print(\"max_i_idL\",max_i_id)\n","\n","    return max_u_id,max_i_id,user_ratings\n","\n","def generate_test(user_ratings):\n","    \"\"\"\n","    对每一个用户u，在user_ratings中随机找到他评分过的一部电影i,保存在user_ratings_test，\n","    后面构造训练集和测试集需要用到。\n","    \"\"\"\n","    user_test = dict()\n","    for u,i_list in user_ratings.items():\n","        user_test[u] = random.sample(user_ratings[u],1)[0]\n","    return user_test\n","\n","\n","def generate_train_batch(user_ratings,user_ratings_test,item_count,batch_size=512):\n","    \"\"\"\n","    构造训练用的三元组\n","    对于随机抽出的用户u，i可以从user_ratings随机抽出，而j也是从总的电影集中随机抽出，当然j必须保证(u,j)不在user_ratings中\n","    \"\"\"\n","    t = []\n","    for b in range(batch_size):\n","        u = random.sample(user_ratings.keys(),1)[0]\n","        i = random.sample(user_ratings[u],1)[0]\n","        while i==user_ratings_test[u]:\n","            i = random.sample(user_ratings[u],1)[0]\n","\n","        j = random.randint(1,item_count)\n","        while j in user_ratings[u]:\n","            j = random.randint(1,item_count)\n","\n","        t.append([u,i,j])\n","\n","    return np.asarray(t)\n","\n","\n","def generate_test_batch(user_ratings,user_ratings_test,item_count):\n","    \"\"\"\n","    对于每个用户u，它的评分电影i是我们在user_ratings_test中随机抽取的，它的j是用户u所有没有评分过的电影集合，\n","    比如用户u有1000部电影没有评分，那么这里该用户的测试集样本就有1000个\n","    \"\"\"\n","    for u in user_ratings.keys():\n","        t = []\n","        i = user_ratings_test[u]\n","        for j in range(1,item_count + 1):\n","            if not(j in user_ratings[u]):\n","                t.append([u,i,j])\n","        yield np.asarray(t)\n","        \n","        \n","  \n","\n","def bpr_mf(user_count,item_count,hidden_dim):\n","    u = tf.placeholder(tf.int32,[None])\n","    i = tf.placeholder(tf.int32,[None])\n","    j = tf.placeholder(tf.int32,[None])\n","\n","    user_emb_w = tf.get_variable(\"user_emb_w111\", [user_count + 1, hidden_dim],\n","                                 initializer=tf.random_normal_initializer(0, 0.1))\n","    item_emb_w = tf.get_variable(\"item_emb_w111\", [item_count + 1, hidden_dim],\n","                                 initializer=tf.random_normal_initializer(0, 0.1))\n","\n","    u_emb = tf.nn.embedding_lookup(user_emb_w, u)\n","    i_emb = tf.nn.embedding_lookup(item_emb_w, i)\n","    j_emb = tf.nn.embedding_lookup(item_emb_w, j)\n","\n","\n","    x = tf.reduce_sum(tf.multiply(u_emb,(i_emb-j_emb)),1,keepdims=True)\n","\n","    mf_auc = tf.reduce_mean(tf.to_float(x>0))\n","\n","    l2_norm = tf.add_n([\n","        tf.reduce_sum(tf.multiply(u_emb, u_emb)),\n","        tf.reduce_sum(tf.multiply(i_emb, i_emb)),\n","        tf.reduce_sum(tf.multiply(j_emb, j_emb))\n","    ])\n","\n","    regulation_rate = 0.001\n","    bprloss = regulation_rate * l2_norm - tf.reduce_mean(tf.log(tf.sigmoid(x)))\n","\n","    train_op = tf.train.GradientDescentOptimizer(0.02).minimize(bprloss)\n","    return u, i, j, mf_auc, bprloss, train_op\n","\n","\n","user_count,item_count,user_ratings = load_data()\n","user_ratings_test = generate_test(user_ratings)\n","\n","with tf.Session() as sess:\n","    u,i,j,mf_auc,bprloss,train_op = bpr_mf(user_count,item_count,60)\n","    sess.run(tf.global_variables_initializer())\n","    \n","    _min_bpr_test_loss=4 #记录testloss最小点\n","    raise_times = 0\n","\n","    for epoch in range(1,90):\n","        _batch_bprloss = 0\n","        for k in range(1,5000):\n","            uij = generate_train_batch(user_ratings,user_ratings_test,item_count)\n","            _bprloss,_train_op = sess.run([bprloss,train_op],\n","                                          feed_dict={u:uij[:,0],i:uij[:,1],j:uij[:,2]})\n","\n","            _batch_bprloss += _bprloss\n","\n","        print(\"epoch:\",epoch)\n","        print(\"bpr_loss:\",_batch_bprloss / k)\n","        print(\"_train_op\")\n","\n","        user_count = 0\n","        _auc_sum = 0.0\n","\n","        for t_uij in generate_test_batch(user_ratings, user_ratings_test, item_count):\n","            _auc, _test_bprloss = sess.run([mf_auc, bprloss],\n","                                              feed_dict={u: t_uij[:, 0], i: t_uij[:, 1], j: t_uij[:, 2]}\n","                                              )\n","            user_count += 1\n","            _auc_sum += _auc\n","        print(\"test_loss: \", _test_bprloss, \"test_auc: \", _auc_sum / user_count)\n","        print(\"\")\n","        \n","        _min_bpr_test_loss = min(_min_bpr_test_loss, _test_bprloss)\n","        \n","        if(_test_bprloss > _min_bpr_test_loss and raise_times>5):\n","            raise_times += 1\n","            break\n","        \n","        \n","        \n","    variable_names = [v.name for v in tf.trainable_variables()]\n","    values = sess.run(variable_names)\n","    for k, v in zip(variable_names, values):\n","        print(\"Variable: \", k)\n","        print(\"Shape: \", v.shape)\n","        print(v)\n","\n","#  0号用户对这个用户对所有电影的预测评分\n","session1 = tf.Session()\n","u1_dim = tf.expand_dims(values[0][0], 0)\n","u1_all = tf.matmul(u1_dim, values[1],transpose_b=True)\n","result_1 = session1.run(u1_all)\n","print (result_1)\n","\n","print(\"以下是给用户0的推荐：\")\n","p = np.squeeze(result_1)\n","p[np.argsort(p)[:-5]] = 0\n","for index in range(len(p)):\n","    if p[index] != 0:\n","        print (index, p[index])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["('max_u_id:', 943)\n","('max_i_idL', 1682)\n","('epoch:', 1)\n","('bpr_loss:', 1.4805174798196639)\n","_train_op\n","('test_loss: ', 2.7497492, 'test_auc: ', 0.5128121439412963)\n","\n","('epoch:', 2)\n","('bpr_loss:', 1.2872628253515923)\n","_train_op\n","('test_loss: ', 2.339624, 'test_auc: ', 0.5117807188155258)\n","\n","('epoch:', 3)\n","('bpr_loss:', 1.1575507294490208)\n","_train_op\n","('test_loss: ', 2.0184846, 'test_auc: ', 0.5107316778081915)\n","\n","('epoch:', 4)\n","('bpr_loss:', 1.0630104006421783)\n","_train_op\n","('test_loss: ', 1.7635605, 'test_auc: ', 0.510025924227713)\n","\n","('epoch:', 5)\n","('bpr_loss:', 0.9910635721161261)\n","_train_op\n","('test_loss: ', 1.5613501, 'test_auc: ', 0.5092581597391135)\n","\n","('epoch:', 6)\n","('bpr_loss:', 0.9351555270227248)\n","_train_op\n","('test_loss: ', 1.3996243, 'test_auc: ', 0.5087528670878739)\n","\n","('epoch:', 7)\n","('bpr_loss:', 0.8909926547673159)\n","_train_op\n","('test_loss: ', 1.2702365, 'test_auc: ', 0.5081569385595821)\n","\n","('epoch:', 8)\n","('bpr_loss:', 0.8555766337154913)\n","_train_op\n","('test_loss: ', 1.1648606, 'test_auc: ', 0.5077204946917407)\n","\n","('epoch:', 9)\n","('bpr_loss:', 0.8270763420753418)\n","_train_op\n","('test_loss: ', 1.0797554, 'test_auc: ', 0.5075063292719305)\n","\n","('epoch:', 10)\n","('bpr_loss:', 0.8038974158118596)\n","_train_op\n","('test_loss: ', 1.0116242, 'test_auc: ', 0.5074239653507534)\n","\n","('epoch:', 11)\n","('bpr_loss:', 0.7850634695458684)\n","_train_op\n","('test_loss: ', 0.9558733, 'test_auc: ', 0.5073047600171015)\n","\n","('epoch:', 12)\n","('bpr_loss:', 0.7696327847059928)\n","_train_op\n","('test_loss: ', 0.9102705, 'test_auc: ', 0.5073561100012693)\n","\n","('epoch:', 13)\n","('bpr_loss:', 0.7568752538635626)\n","_train_op\n","('test_loss: ', 0.87256765, 'test_auc: ', 0.5074262786029482)\n","\n","('epoch:', 14)\n","('bpr_loss:', 0.7464298980549399)\n","_train_op\n","('test_loss: ', 0.8422421, 'test_auc: ', 0.5075697505408118)\n","\n","('epoch:', 15)\n","('bpr_loss:', 0.7377721510378926)\n","_train_op\n","('test_loss: ', 0.8171312, 'test_auc: ', 0.5076708644322364)\n","\n","('epoch:', 16)\n","('bpr_loss:', 0.7305961808435295)\n","_train_op\n","('test_loss: ', 0.7963241, 'test_auc: ', 0.5078412756701084)\n","\n","('epoch:', 17)\n","('bpr_loss:', 0.7246235153464752)\n","_train_op\n","('test_loss: ', 0.7791585, 'test_auc: ', 0.5081372162986898)\n","\n","('epoch:', 18)\n","('bpr_loss:', 0.7196532255364647)\n","_train_op\n","('test_loss: ', 0.76525134, 'test_auc: ', 0.508588835522967)\n","\n","('epoch:', 19)\n","('bpr_loss:', 0.715509406743848)\n","_train_op\n","('test_loss: ', 0.75373286, 'test_auc: ', 0.5088843471673836)\n","\n","('epoch:', 20)\n","('bpr_loss:', 0.7120507227275151)\n","_train_op\n","('test_loss: ', 0.7441484, 'test_auc: ', 0.5092913528009538)\n","\n","('epoch:', 21)\n","('bpr_loss:', 0.7091545988188002)\n","_train_op\n","('test_loss: ', 0.736139, 'test_auc: ', 0.5096562903982805)\n","\n","('epoch:', 22)\n","('bpr_loss:', 0.7066975657189696)\n","_train_op\n","('test_loss: ', 0.72946125, 'test_auc: ', 0.5101904849090227)\n","\n","('epoch:', 23)\n","('bpr_loss:', 0.7046564507780134)\n","_train_op\n","('test_loss: ', 0.7238691, 'test_auc: ', 0.5107493980423264)\n","\n","('epoch:', 24)\n","('bpr_loss:', 0.7029276046068055)\n","_train_op\n","('test_loss: ', 0.71915114, 'test_auc: ', 0.5112897268045113)\n","\n","('epoch:', 25)\n","('bpr_loss:', 0.7014724511603256)\n","_train_op\n","('test_loss: ', 0.7152109, 'test_auc: ', 0.51196052996819)\n","\n","('epoch:', 26)\n","('bpr_loss:', 0.7002439184722054)\n","_train_op\n","('test_loss: ', 0.7119257, 'test_auc: ', 0.5127033459601384)\n","\n","('epoch:', 27)\n","('bpr_loss:', 0.6992008860146053)\n","_train_op\n","('test_loss: ', 0.70914423, 'test_auc: ', 0.513544602098372)\n","\n","('epoch:', 28)\n","('bpr_loss:', 0.6983145168958413)\n","_train_op\n","('test_loss: ', 0.7068093, 'test_auc: ', 0.5143557029103708)\n","\n","('epoch:', 29)\n","('bpr_loss:', 0.6975662616711422)\n","_train_op\n","('test_loss: ', 0.7048158, 'test_auc: ', 0.5153651211197435)\n","\n","('epoch:', 30)\n","('bpr_loss:', 0.696931074025226)\n","_train_op\n","('test_loss: ', 0.7031262, 'test_auc: ', 0.5165223356984232)\n","\n","('epoch:', 31)\n","('bpr_loss:', 0.6963854362116167)\n","_train_op\n","('test_loss: ', 0.7016862, 'test_auc: ', 0.5176875919403858)\n","\n","('epoch:', 32)\n","('bpr_loss:', 0.6959249163227192)\n","_train_op\n","('test_loss: ', 0.70046073, 'test_auc: ', 0.5190547760694524)\n","\n","('epoch:', 33)\n","('bpr_loss:', 0.6955326902053194)\n","_train_op\n","('test_loss: ', 0.6994265, 'test_auc: ', 0.5203488724798059)\n","\n","('epoch:', 34)\n","('bpr_loss:', 0.695197341966257)\n","_train_op\n","('test_loss: ', 0.6985406, 'test_auc: ', 0.521894243183375)\n","\n","('epoch:', 35)\n","('bpr_loss:', 0.6949093023236452)\n","_train_op\n","('test_loss: ', 0.69778556, 'test_auc: ', 0.5235288574450184)\n","\n","('epoch:', 36)\n","('bpr_loss:', 0.6946650148391914)\n","_train_op\n","('test_loss: ', 0.69714266, 'test_auc: ', 0.5251995960343333)\n","\n"],"name":"stdout"}]}]}