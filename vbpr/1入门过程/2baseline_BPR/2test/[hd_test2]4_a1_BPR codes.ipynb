{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[hd_test2]4_a1_BPR codes.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python2","display_name":"Python 2"}},"cells":[{"metadata":{"id":"lZqDtj3nbx0X","colab_type":"code","outputId":"39380515-7e6e-4224-9d9a-ba655fcc8907","executionInfo":{"status":"ok","timestamp":1552554205330,"user_tz":-480,"elapsed":19850,"user":{"displayName":"Jacob Ji","photoUrl":"https://lh3.googleusercontent.com/-zoRekIA1fCY/AAAAAAAAAAI/AAAAAAAACBQ/eV12vlBRQiA/s64/photo.jpg","userId":"06638368627777389358"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"cell_type":"code","source":["!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 授权登录，仅第一次的时候会鉴权\n","def login_google_drive():\n","  auth.authenticate_user()\n","  gauth = GoogleAuth()\n","  gauth.credentials = GoogleCredentials.get_application_default()\n","  drive = GoogleDrive(gauth)\n","  return drive\n","\n","# 列出id对应目录的所有文件\n","# \"q\" 查询条件教程详见：https://developers.google.com/drive/v2/web/search-parameters\n","def list_file(drive, dir_id_str):\n","  file_list = drive.ListFile({'q': dir_id_str+\" in parents and trashed=false\"}).GetList()\n","  for file1 in file_list:\n","    print('title: %s, id: %s, mimeType: %s' % (file1['title'], file1['id'], file1[\"mimeType\"]))\n","\n","def cache_data(file_id_str):\n","  # id 替换成上一步读取到的对应文件 id\n","  u_data = drive.CreateFile({'id': file_id_str}) \n","  \n","  #这里的下载操作只是缓存，不会在你的Google Drive 目录下多下载一个文件\n","  u_data.GetContentFile('u.data', \"text/csv\")\n","  print(\"缓存成功\")\n","\n","def load_data():\n","    titles = []\n","    print(\"正在加载数据...\")\n","    with open(\"u.data\", \"r\") as f:\n","        for line in f.readlines():\n","            titles.append(line.strip())\n","            \n","    print(\"一共加载了 %s 个标题\" % len(titles))\n","\n","    return titles\n","  \n","drive = login_google_drive()\n","#list_file(drive,\"'1DCdKTVKMGR_ei-NWE-68Y_mRFPf5RQhv'\")\n","cache_data(\"14Eha5chGz6RNWIYALvZBGI8Q-yAUmj7l\")\n","titles = load_data()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["缓存成功\n","正在加载数据...\n","一共加载了 100000 个标题\n"],"name":"stdout"}]},{"metadata":{"id":"Mz3-SK9pcL0E","colab_type":"text"},"cell_type":"markdown","source":["## 备注：修改参数：\n","- BatchSize=256：尽量充分利用内存\n","- LR=0.02：下调学习速率，能更好抓住最优解，之后考虑加入动态调节学习速率的方法\n","- RR=0.001：\n","- hidden_dims = 20\n","- k=5000：充分调用数据"]},{"metadata":{"id":"faJtWYqxdhY2","colab_type":"code","outputId":"893e20e7-c0f4-4867-98fa-8fde58856ee0","colab":{"base_uri":"https://localhost:8080/","height":2798}},"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import os\n","import random\n","from collections import defaultdict\n","\n","\n","def load_data():\n","    user_ratings = defaultdict(set)\n","    max_u_id = -1\n","    max_i_id = -1\n","    with open('u.data','r') as f:\n","        for line in f.readlines():\n","            u,i,_,_ = line.split(\"\\t\")\n","            u = int(u)\n","            i = int(i)\n","            user_ratings[u].add(i)\n","            max_u_id = max(u,max_u_id)\n","            max_i_id = max(i,max_i_id)\n","\n","\n","    print(\"max_u_id:\",max_u_id)\n","    print(\"max_i_idL\",max_i_id)\n","\n","    return max_u_id,max_i_id,user_ratings\n","\n","def generate_test(user_ratings):\n","    \"\"\"\n","    对每一个用户u，在user_ratings中随机找到他评分过的一部电影i,保存在user_ratings_test，\n","    后面构造训练集和测试集需要用到。\n","    \"\"\"\n","    user_test = dict()\n","    for u,i_list in user_ratings.items():\n","        user_test[u] = random.sample(user_ratings[u],1)[0]\n","    return user_test\n","\n","\n","def generate_train_batch(user_ratings,user_ratings_test,item_count,batch_size=512):\n","    \"\"\"\n","    构造训练用的三元组\n","    对于随机抽出的用户u，i可以从user_ratings随机抽出，而j也是从总的电影集中随机抽出，当然j必须保证(u,j)不在user_ratings中\n","    \"\"\"\n","    t = []\n","    for b in range(batch_size):\n","        u = random.sample(user_ratings.keys(),1)[0]\n","        i = random.sample(user_ratings[u],1)[0]\n","        while i==user_ratings_test[u]:\n","            i = random.sample(user_ratings[u],1)[0]\n","\n","        j = random.randint(1,item_count)\n","        while j in user_ratings[u]:\n","            j = random.randint(1,item_count)\n","\n","        t.append([u,i,j])\n","\n","    return np.asarray(t)\n","\n","\n","def generate_test_batch(user_ratings,user_ratings_test,item_count):\n","    \"\"\"\n","    对于每个用户u，它的评分电影i是我们在user_ratings_test中随机抽取的，它的j是用户u所有没有评分过的电影集合，\n","    比如用户u有1000部电影没有评分，那么这里该用户的测试集样本就有1000个\n","    \"\"\"\n","    for u in user_ratings.keys():\n","        t = []\n","        i = user_ratings_test[u]\n","        for j in range(1,item_count + 1):\n","            if not(j in user_ratings[u]):\n","                t.append([u,i,j])\n","        yield np.asarray(t)\n","        \n","        \n","  \n","\n","def bpr_mf(user_count,item_count,hidden_dim):\n","    u = tf.placeholder(tf.int32,[None])\n","    i = tf.placeholder(tf.int32,[None])\n","    j = tf.placeholder(tf.int32,[None])\n","\n","    user_emb_w = tf.get_variable(\"user_emb_w11\", [user_count + 1, hidden_dim],\n","                                 initializer=tf.random_normal_initializer(0, 0.1))\n","    item_emb_w = tf.get_variable(\"item_emb_w11\", [item_count + 1, hidden_dim],\n","                                 initializer=tf.random_normal_initializer(0, 0.1))\n","\n","    u_emb = tf.nn.embedding_lookup(user_emb_w, u)\n","    i_emb = tf.nn.embedding_lookup(item_emb_w, i)\n","    j_emb = tf.nn.embedding_lookup(item_emb_w, j)\n","\n","\n","    x = tf.reduce_sum(tf.multiply(u_emb,(i_emb-j_emb)),1,keepdims=True)\n","\n","    mf_auc = tf.reduce_mean(tf.to_float(x>0))\n","\n","    l2_norm = tf.add_n([\n","        tf.reduce_sum(tf.multiply(u_emb, u_emb)),\n","        tf.reduce_sum(tf.multiply(i_emb, i_emb)),\n","        tf.reduce_sum(tf.multiply(j_emb, j_emb))\n","    ])\n","\n","    regulation_rate = 0.001\n","    bprloss = regulation_rate * l2_norm - tf.reduce_mean(tf.log(tf.sigmoid(x)))\n","\n","    train_op = tf.train.GradientDescentOptimizer(0.02).minimize(bprloss)\n","    return u, i, j, mf_auc, bprloss, train_op\n","\n","\n","user_count,item_count,user_ratings = load_data()\n","user_ratings_test = generate_test(user_ratings)\n","\n","with tf.Session() as sess:\n","    u,i,j,mf_auc,bprloss,train_op = bpr_mf(user_count,item_count,20)\n","    sess.run(tf.global_variables_initializer())\n","    \n","    _min_bpr_test_loss=1 #记录testloss最小点\n","    raise_times = 0\n","\n","    for epoch in range(1,90):\n","        _batch_bprloss = 0\n","        for k in range(1,5000):\n","            uij = generate_train_batch(user_ratings,user_ratings_test,item_count)\n","            _bprloss,_train_op = sess.run([bprloss,train_op],\n","                                          feed_dict={u:uij[:,0],i:uij[:,1],j:uij[:,2]})\n","\n","            _batch_bprloss += _bprloss\n","\n","        print(\"epoch:\",epoch)\n","        print(\"bpr_loss:\",_batch_bprloss / k)\n","        print(\"_train_op\")\n","\n","        user_count = 0\n","        _auc_sum = 0.0\n","\n","        for t_uij in generate_test_batch(user_ratings, user_ratings_test, item_count):\n","            _auc, _test_bprloss = sess.run([mf_auc, bprloss],\n","                                              feed_dict={u: t_uij[:, 0], i: t_uij[:, 1], j: t_uij[:, 2]}\n","                                              )\n","            user_count += 1\n","            _auc_sum += _auc\n","        print(\"test_loss: \", _test_bprloss, \"test_auc: \", _auc_sum / user_count)\n","        print(\"\")\n","        \n","        _min_bpr_test_loss = min(_min_bpr_test_loss, _test_bprloss)\n","        \n","        if(_test_bprloss > _min_bpr_test_loss and raise_times>5):\n","            raise_times += 1\n","            break\n","        \n","        \n","        \n","    variable_names = [v.name for v in tf.trainable_variables()]\n","    values = sess.run(variable_names)\n","    for k, v in zip(variable_names, values):\n","        print(\"Variable: \", k)\n","        print(\"Shape: \", v.shape)\n","        print(v)\n","\n","#  0号用户对这个用户对所有电影的预测评分\n","session1 = tf.Session()\n","u1_dim = tf.expand_dims(values[0][0], 0)\n","u1_all = tf.matmul(u1_dim, values[1],transpose_b=True)\n","result_1 = session1.run(u1_all)\n","print (result_1)\n","\n","print(\"以下是给用户0的推荐：\")\n","p = np.squeeze(result_1)\n","p[np.argsort(p)[:-5]] = 0\n","for index in range(len(p)):\n","    if p[index] != 0:\n","        print (index, p[index])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["('max_u_id:', 943)\n","('max_i_idL', 1682)\n","('epoch:', 1)\n","('bpr_loss:', 0.9577802369465325)\n","_train_op\n","('test_loss: ', 1.3159077, 'test_auc: ', 0.49866477044831076)\n","\n","('epoch:', 2)\n","('bpr_loss:', 0.8921611190414543)\n","_train_op\n","('test_loss: ', 1.1858435, 'test_auc: ', 0.49810489110832823)\n","\n","('epoch:', 3)\n","('bpr_loss:', 0.8485484748488547)\n","_train_op\n","('test_loss: ', 1.0864938, 'test_auc: ', 0.4979482425399241)\n","\n","('epoch:', 4)\n","('bpr_loss:', 0.8168447353549423)\n","_train_op\n","('test_loss: ', 1.0085154, 'test_auc: ', 0.4980639344154613)\n","\n","('epoch:', 5)\n","('bpr_loss:', 0.7928156602690282)\n","_train_op\n","('test_loss: ', 0.9479299, 'test_auc: ', 0.49818078303647406)\n","\n","('epoch:', 6)\n","('bpr_loss:', 0.7740548839090251)\n","_train_op\n","('test_loss: ', 0.8991509, 'test_auc: ', 0.49823550686024404)\n","\n","('epoch:', 7)\n","('bpr_loss:', 0.7592514171483971)\n","_train_op\n","('test_loss: ', 0.86033386, 'test_auc: ', 0.49855598776187937)\n","\n","('epoch:', 8)\n","('bpr_loss:', 0.7473980626432293)\n","_train_op\n","('test_loss: ', 0.82899976, 'test_auc: ', 0.498829843538758)\n","\n","('epoch:', 9)\n","('bpr_loss:', 0.7378461477302938)\n","_train_op\n","('test_loss: ', 0.80422986, 'test_auc: ', 0.4990746509485008)\n","\n","('epoch:', 10)\n","('bpr_loss:', 0.7301216712234544)\n","_train_op\n","('test_loss: ', 0.7844521, 'test_auc: ', 0.49947967948794486)\n","\n","('epoch:', 11)\n","('bpr_loss:', 0.7238073943256974)\n","_train_op\n","('test_loss: ', 0.76837695, 'test_auc: ', 0.4996501277456423)\n","\n","('epoch:', 12)\n","('bpr_loss:', 0.7186434973738484)\n","_train_op\n","('test_loss: ', 0.75552636, 'test_auc: ', 0.5000266734898177)\n","\n","('epoch:', 13)\n","('bpr_loss:', 0.7143981272899096)\n","_train_op\n","('test_loss: ', 0.74494654, 'test_auc: ', 0.5003469334743682)\n","\n","('epoch:', 14)\n","('bpr_loss:', 0.7109048340196108)\n","_train_op\n","('test_loss: ', 0.73623204, 'test_auc: ', 0.5007573950336752)\n","\n","('epoch:', 15)\n","('bpr_loss:', 0.7080089341380353)\n","_train_op\n","('test_loss: ', 0.7291466, 'test_auc: ', 0.5012068970944665)\n","\n","('epoch:', 16)\n","('bpr_loss:', 0.7056108753808524)\n","_train_op\n","('test_loss: ', 0.7232466, 'test_auc: ', 0.5016239873041751)\n","\n","('epoch:', 17)\n","('bpr_loss:', 0.7036263435262279)\n","_train_op\n","('test_loss: ', 0.71837556, 'test_auc: ', 0.5020618002174357)\n","\n","('epoch:', 18)\n","('bpr_loss:', 0.7019661854185756)\n","_train_op\n","('test_loss: ', 0.7143497, 'test_auc: ', 0.502547962091318)\n","\n","('epoch:', 19)\n","('bpr_loss:', 0.7005826653921978)\n","_train_op\n","('test_loss: ', 0.7110186, 'test_auc: ', 0.5029908364395732)\n","\n","('epoch:', 20)\n","('bpr_loss:', 0.6994311252244116)\n","_train_op\n","('test_loss: ', 0.70825166, 'test_auc: ', 0.5034146623958539)\n","\n","('epoch:', 21)\n","('bpr_loss:', 0.6984633969220907)\n","_train_op\n","('test_loss: ', 0.7059298, 'test_auc: ', 0.5038802696973153)\n","\n","('epoch:', 22)\n","('bpr_loss:', 0.6976526941173338)\n","_train_op\n","('test_loss: ', 0.70398426, 'test_auc: ', 0.5043735850115)\n","\n","('epoch:', 23)\n","('bpr_loss:', 0.6969679484822364)\n","_train_op\n","('test_loss: ', 0.7023583, 'test_auc: ', 0.5049251383128511)\n","\n","('epoch:', 24)\n","('bpr_loss:', 0.6963933435982049)\n","_train_op\n","('test_loss: ', 0.70099425, 'test_auc: ', 0.5056084150511155)\n","\n","('epoch:', 25)\n","('bpr_loss:', 0.6959101577285862)\n","_train_op\n","('test_loss: ', 0.69983625, 'test_auc: ', 0.5062032373834892)\n","\n","('epoch:', 26)\n","('bpr_loss:', 0.6954991815566635)\n","_train_op\n","('test_loss: ', 0.69886416, 'test_auc: ', 0.5069411825810876)\n","\n","('epoch:', 27)\n","('bpr_loss:', 0.695154515367242)\n","_train_op\n","('test_loss: ', 0.6980388, 'test_auc: ', 0.5076178617294965)\n","\n","('epoch:', 28)\n","('bpr_loss:', 0.6948614342614731)\n","_train_op\n","('test_loss: ', 0.69733995, 'test_auc: ', 0.5083682015851784)\n","\n","('epoch:', 29)\n","('bpr_loss:', 0.6946104307321578)\n","_train_op\n","('test_loss: ', 0.6967446, 'test_auc: ', 0.5092641560655593)\n","\n","('epoch:', 30)\n","('bpr_loss:', 0.6944003954699669)\n","_train_op\n","('test_loss: ', 0.69623333, 'test_auc: ', 0.5101088305981187)\n","\n","('epoch:', 31)\n","('bpr_loss:', 0.6942198301677968)\n","_train_op\n","('test_loss: ', 0.6958002, 'test_auc: ', 0.5112084887770211)\n","\n"],"name":"stdout"}]}]}