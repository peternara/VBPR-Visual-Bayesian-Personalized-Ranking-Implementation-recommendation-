{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在网上搜索如何上手看论文的时候，搜到了一个大佬刚好有在[解说这篇文章](http://blog.cheny.org/paper-bpr/)。\n",
    ">题外话：*顺便记录了一波自己的心路历程，也蛮搞笑的。顺便扫了一波他的其他post。woc他竟然才大一就已经开始上手这些了。好像还接了中科院的什么推荐系统项目。发现他是北邮的。他的导师嫌弃他说人家高中生都一周可以一篇a类顶会了。woc他六年级就已经在开始自己开发网站论坛了，上线的一系列论坛版块功能很完善，自己设计UI。woc初一就开始关注谷歌推出的知识图谱了。还顺便记录了一波自己的产品创意。。woc这个人学术、工程、设计、文学社科都有涉猎。在初二好像还开始连载自己写的小说。。另外还学了画画，读了一些哲学开始阐述自己的心得。woc这个人竟然还有女朋友。真滴强。鼓掌。*<br>\n",
    "\n",
    "**※补**：后面看到另外一篇[解说](http://lipixun.me/2018/01/22/bpr)的也不错，更详细\n",
    "\n",
    "---\n",
    "算了不管了。再晚也得开始。继续看我的论文吧。看看人家看这个论文的时候，思路是什么。\n",
    "论正确的论文入坑姿势，想起来国立清华大学彭明辉老师写的[研究手册](www2.kuas.edu.tw/prof/sheen/team/04.pdf)（对我这种小白，真的已经良心哭）\n",
    "\n",
    "---\n",
    "直接从头开始看起。感觉学了三年，数学也跟白学了一样。学完了就还给了老师。好烦啊啊啊啊感觉自己好废orz<br>\n",
    "从Abstract看起，把论文中提到的，自己不会的概念先圈起来再说。\n",
    "- ✔什么是Matrix Factorization；什么是Stochastic Gradient Descent随机梯度下降，那基于bootstrap sampling 的随机梯度下降是啥，Singular Value Decomposition奇异值分解又是啥哇；\n",
    "    >- 梯度下降法Gradient descent:https://www.cnblogs.com/pinard/p/5970503.html 介绍很详细，帮助回忆了一遍。注意批量梯度下降（Batch Gradient Descent），随机梯度下降法(Stochastic Gradient Descent)，随机梯度下降是选区一个样本来求梯度。\n",
    "    >- 训练速度上，随机梯度下降更快；准确度和收敛速度上，批量梯度下降表现更优。\n",
    "    >- **Bootstrap Sampling**:<br>\n",
    "    为了理解boostrap 抽样，先把置信区间复习理解了一遍：👉[知乎回答](https://www.zhihu.com/question/26419030/answer/274472266) 讲的不错。<br>然后[这个博客](https://blog.csdn.net/iterate7/article/details/79740136 )解释的比较详细：\n",
    "    它是一种有放回的抽样方法，它是非参数统计中一种重要的估计统计量方差进而进行区间估计的统计方法。其核心思想和基本步骤如下： \n",
    "          （1） 采用重抽样技术从原始样本中抽取一定数量（自己给定）的样本，此过程允许重复抽样。 \n",
    "          （2） 根据抽出的样本计算给定的统计量T。 \n",
    "          （3） 重复上述N次（一般大于1000），得到N个统计量T。 \n",
    "          （4） 计算上述N个统计量T的样本方差，得到统计量的方差。\n",
    "  <br>[这里](http://blog.cheny.org/paper-bpr/)还有通俗的解释：Bootstrap抽样的基本思想是在全部样本未知的情况下，借助部分样本的有放回多次抽样，构建某个估计的置信区间，抽象地说，通过样本得到的估计并没有榨干样本中的信息，bootstrap利用重采样，把剩余价值发挥在构建置信区间上。\n",
    "  >- 矩阵分解Matrix Factorization：这里分别有一篇讲的特别好（适合菜鸟入门）的[笔记](http://www.albertauyeung.com/post/python-matrix-factorization/)和[YouTube视频](https://youtu.be/ZspR5PZemcs)，其中提到过拟合的概念，可以顺带再复习一遍吴恩达老师的[视频](https://youtu.be/u73PU6Qwl1I)(真的是专业菜鸟了==捂脸，各种概念提到一个就得学提到一个就得学..\n",
    "  >- 奇异值分解Singular Value Decomposition：目前还不太明白SVD的具体变换过程。\n",
    "    > 待填坑（如果这里不懂也能实现就先略过吧==，不过这里有一段斯坦福关于SVD的[讲解课程](https://youtu.be/P5mlg91as1c)，先马上。用到的时候再看嘻嘻。\n",
    "  \n",
    "- ✔什么是kNN，和上面的MF好像是平行的方法，都用来流行的应用算法；\n",
    "    >- 其实实际采用的cosine近似法，就是在前一本书里提到的基于物品的协同过滤算法（item-item）\n",
    "    $$ 先建立一个item-based的矩阵： C: I * I $$\n",
    "    $$ c_{i,j} = \\frac{|U^+_i\\cap U^+_j|}{\\sqrt{|U^+_i|·|U^+_j|}} $$\n",
    "    kNN的$\\Theta$就是C\n",
    "- ✔ 什么是贝叶斯，什么是先验概率，后验概率,协方差...听到了数学老师的哭声。\n",
    "    >- 贝叶斯 Bayes' Theorem： $P(y|x)=\\frac{P(x|y)P(y)}{P(x)} $\n",
    "    >- 先验概率priori probability：事件发生前的预判概率。可以是基于历史数据的统计，可以由背景常识得出，也可以是人的主观观点给出。一般都是单独事件概率，如$P(x),P(y)$。\n",
    "    >- 后验概率posterior probability：先知道结果，然后由结果估计原因的概率分布，就是后验概率；或者说，基于先验概率求得的反向条件概率。概率形式与条件概率相同,如$P(y|x)$。\n",
    "    >- 条件概率：一个事件发生后另一个事件发生的概率。一般的形式为$P(x|y)$表示y发生的条件下x发生的概率。\n",
    "    >- 假设集合A，以及基于A上的关系R\n",
    "          反对称antisymmetry：如果<a,b>，<b,a>是R的元素，那么a,b相等 \n",
    "          传递transitivity：  如果<a,b>，<b,c>是R的元素，那么<a,c>是R的元素\n",
    "    >- 方差-协方差矩阵variance-covariance matrix：看完[维基](https://zh.wikipedia.org/wiki/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5)也比较好理解了。\n",
    "\n",
    "- 什么是Ranking，看到这个[博客《贝叶斯个性化排序(BPR)算法小结》](http://www.cnblogs.com/pinard/p/9128682.html)推荐了一个李航老师写的ranking的[入门](http://times.cs.uiuc.edu/course/598f13/l2r.pdf),先下载了。里面还提到**Pairwise/Pointwise**，注意也要搞清是什么；\n",
    "    >- Rank：科普入门，挖个[小坑](#Rank)👇\n",
    "- 在讨论编程实现的时候，他提到了一些工具numpy，TensorFlow,PyTorch,都是干啥的，怎么用，咦顺便还有Anaconda(正在写笔记用的这玩意儿，也得研究一下；\n",
    "    >- Tensorflow：太多了，挖[坑](#Tensorflow)👇（隐约觉得这又是一个大坑，哇的一声哭出来\n",
    "\n",
    "- ✔ 还得研究一下论文中用到的各种评价性能指标，什么是AUC；\n",
    "    > AUC：**是二分类器中用来反应对正样本，负样本的综合预测能力，还要考虑消除样本倾斜的影响**。简单来说，auc取值范围[0.5,1]，越大表示越好，小于0.5的把结果取反就行。其实是通过真阳性率(TPR)和伪阳性率(FPR）来反映的。具体博客[说明](https://blog.csdn.net/Dinosoft/article/details/43114935)\n",
    "- ✔ 不行，我得先去下载个[数据集](http://grouplens.org/datasets/movielens/),额，那就选09年10M的那个版本吧，还得研究一下数据集里的各项指标。这里也可以用[这里](https://zhuanlan.zhihu.com/p/38099602)提到的数据集（方便和他对照检验自己的结果）。\n",
    "- 他还提到了一个国内已经实现这些经典算法的[java库](https://github.com/guoguibing/librec/tree/3.0.0-beta/core/src/main/java/net/librec/recommender/cf/ranking),这是[关于LibRec库的说明](http://wiki.jikexueyuan.com/project/arithmetic/AlgorithmList.html)\n",
    "要是自己不会了，先[参考这个](https://github.com/guoguibing/librec/blob/3.0.0-beta/core/src/main/java/net/librec/recommender/cf/ranking/BPRRecommender.java)学学人家。好像在coursera中还提到另外一个实现推荐算法的java库[LensKit](https://java.lenskit.org/documentation/basics/getting-started/)。今天晚上回去记得注释这个代码，先过一遍。一定要赶紧实现了！！不然你真的太菜了吧。都这么多天了！加油啊！不能再懒了（心酸的低头看一看座位上掉的头发..)\n",
    "- ✔ 不行，我现在已经把论文中除了重点小节（4）都看完了==，就是4中冒出的各种公式，一直无法投入看进去，吃晚饭的时候买一个小本，用来当草稿纸，一直看一遍手写就难一点跑神了吧==吃完饭先照着上面的几个概念过一遍，然后开始研究公式。\n",
    "待会还要去把新买的鞋子退掉（已丑哭），取个快递。昨天竟然还是我的生日，同学又寄了礼物给我。\n",
    "- 研究4公式的时候，可以照着几篇写的博客看，[《推荐系统遇上深度学习(二十)--贝叶斯个性化排序(BPR)算法原理及实战》](https://www.jianshu.com/p/ba1936ee0b69),[《贝叶斯个性化排序(BPR)算法小结》](http://www.cnblogs.com/pinard/p/9128682.html),第二个人好像，关于这方面，写的博客都不赖。待会再关注一波。吃饭去。\n",
    "- 除了搞懂数学原理。记得关注输入输出，以及各种变量的意义。可以从刚才的那篇github库实现好的代码上手看起，学这个北邮大佬也做一波注释叭hhh，假装自己还是有进度的！吃饭吃饭。希望进展顺利。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4\n",
    "- 看完4.1，还是不太明白$\\Theta$表示的是什么意思。这里就先理解为是模型的参数集合吧。\n",
    "> $\\Theta$ represents the parameter vector of an arbitrary model class(eg. matrix factorization).<br>\n",
    "> **※补**：后面看到4.3.1中明白了，有对$\\Theta$进行详细的定义，是模型参数，(W,H),反映了用户的潜在偏好和每个物品的前在特征属性。<br>\n",
    "> **※补**：在设置$\\Theta$的先验概率服从正态分布后，对$lnP(\\Theta) =\\lambda||\\Theta||^2 $不是很理解。先不求甚解了，必要的话，这里有一篇数学[证明](https://www.jianshu.com/p/4d562f2c06b8)👉*<从贝叶斯角度看L1及L2正则化>*\n",
    "- 看到BPR-OPT的核心算法变换，数学变换是看明白了，但是具体的数学意义还是没能理解完全。\n",
    "- 今天终于看完了这一部分。送一口气。记得赶紧去实现一遍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 附：\n",
    "### 1 Rank\n",
    "先从Rank看起吧：（为啥要在取快递的时候买那家鸡柳（现在肚子因为劣质油快吐了😭\n",
    "```\n",
    "    //以文本检索为例\n",
    "    f(q,d)   //q代表查询语句，d代表一个文档，f代表评分模型\n",
    "```\n",
    "#### 1.1 Training and Testing\n",
    "\n",
    "    \n",
    "### 2 Tensorflow\n",
    "#### 2.1 什么是Tensor 张量\n",
    "找到👉[博客](https://blog.csdn.net/kansas_lh/article/details/79321234)\n",
    "说白了，Tensor是multi-dimensional arrays of numbers.\n",
    "<center>\n",
    "    <img src=\"03\\Tensor.png\">\n",
    "    Tensor\n",
    "</center>\n",
    "\n",
    "#### Tensorflow了解和上手\n",
    "这里有一篇O'REILLY的[入门讲解](https://www.oreilly.com/learning/hello-tensorflow)<br>\n",
    "这里最好跟着书练><\n",
    "\n",
    "#### 矩阵基础\n",
    "在补习矩阵基础知识，特别是这里涉及到Matrix Factorization的时候，遇到了几本书，感觉在结合CS应用时候用来复习，挺不错的。<br>\n",
    "1.[线性代数及其应用](https://book.douban.com/subject/1425950/)\n",
    "2.[程序员的数学3](https://book.douban.com/subject/26740548/)\n",
    "3.[线性代数应该这样学](https://book.douban.com/subject/26886299/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
